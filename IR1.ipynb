{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "54_HlVu1yy92"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJy5vZ8Gy_m1",
        "outputId": "8f2c0096-7e0f-4046-e117-50f996d6d643"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text for preprocessing\n",
        "text = \"Artificial intelligence is a branch of computer science that aims to create intelligent machines. It has become an essential part of the technology industry.\"\n"
      ],
      "metadata": {
        "id": "ACtFGxMXy_oz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOnE5zqqy_qx",
        "outputId": "b9fee41c-de74-4d75-c8b1-b8d4a7794d1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Artificial', 'intelligence', 'is', 'a', 'branch', 'of', 'computer', 'science', 'that', 'aims', 'to', 'create', 'intelligent', 'machines', '.', 'It', 'has', 'become', 'an', 'essential', 'part', 'of', 'the', 'technology', 'industry', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Remove stop words in a simpler way\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = []\n",
        "\n",
        "for word in tokens:\n",
        "    # Convert to lowercase, check if it's a stop word and if it's alphabetic\n",
        "    if word.lower() not in stop_words and word.isalpha():\n",
        "        filtered_tokens.append(word)\n",
        "\n",
        "print(\"\\nAfter Stop Word Removal:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyMrQayly_u8",
        "outputId": "9d1c1042-5ecb-45df-c42f-c892b2475a97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Stop Word Removal: ['Artificial', 'intelligence', 'branch', 'computer', 'science', 'aims', 'create', 'intelligent', 'machines', 'become', 'essential', 'part', 'technology', 'industry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Apply stemming in a simpler way\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = []\n",
        "\n",
        "for word in filtered_tokens:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    stemmed_tokens.append(stemmed_word)\n",
        "\n",
        "print(\"\\nAfter Stemming:\", stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2BZ2hiFy_xH",
        "outputId": "03c52317-55f1-47a6-b9eb-2adcfa68bf66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Stemming: ['artifici', 'intellig', 'branch', 'comput', 'scienc', 'aim', 'creat', 'intellig', 'machin', 'becom', 'essenti', 'part', 'technolog', 'industri']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the processed words back into a single string\n",
        "processed_text = ' '.join(stemmed_tokens)\n",
        "print(\"\\nProcessed Text:\", processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEeSu2Roy_zO",
        "outputId": "373737fb-7329-4bbd-c91c-7a649389113b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed Text: artifici intellig branch comput scienc aim creat intellig machin becom essenti part technolog industri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9uwaLzNozU_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XeHSy3UXz0FN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}